\section{Introduction}
\label{SEC:introduction}
\textit{No Battle Plan Survives Contact With the Enemy --- Helmuth von Moltke}

No matter how well an application is tested before its release,
new bugs always seem to emerge after deployment.
Oracle estimates that 40\% of deployed applications
contain critical defects -- a situation that is compounded
by the fact that deployment
increases the cost to fix these flaws by 100 fold~\cite{OracleAppQuality}.
One reason for this outcome
is that these applications will operate within a diverse set of
deployment \emph{environments},
and variations between these environments tend to
reveal previously undiscovered flaws.
These flaws emerge from
such factors as
operating system APIs changing across versions
~\cite{LinuxGlibcChanges, WinAPICompat, MuslDifferences},
or small variations in file systems exhibiting subtle but critical
differences~\cite{EXT4Layout, AppleHFS, WindowsNTFS}.
Even if the network and adapter are identical,
network behavior can still diverge from what is expected~\cite{vbox,
NMAPOSDifferences, VMWareNATFailure},
and these environmental differences greatly exacerbate
the chance that an application will function incorrectly when deployed.

These unforeseen bugs
complicate the work of 43\% of application developers who, according to a
recent survey conducted by ClusterHQ~\cite{ClusterHQSurvey},
spend between 10\% and 25\% of their time
debugging errors that only appear in production.
%Participants in this survey cited the inability to recreate
%production environments for
%testing as the main reason why bugs are not discovered earlier.
Numerous efforts have been made to reduce this burden.
One approach
is to hide environmental differences behind standard interfaces.
Unfortunately,
even specialized ``Write-Once, Run Anywhere'' environments
that attempt to hide these differences,
such as the Java Runtime Environment,
are not perfect,
leading them to be rechristened ``Write-Once, Debug Everywhere''~\cite{WODE}.
A more direct approach would be
to identify and fix deficiencies before deployment,
but history has shown that,
even if enormous effort is put forward,
it may be insufficient to uncover these bugs.
Microsoft employs thousands of engineers with nearly a
1:1 ratio of testers to developers~\cite{Page2009}.
Yet, a recent Windows Update released
in response to the Spectre Intel CPU vulnerability
resulted in machines with certain hardware configurations
being rendered unbootable~\cite{kb4056892}.

What is needed is a methodical way to record, preserve, and test against
specific features of any environment proven to have caused incorrect
behavior in applications. We achieve this by cataloging these
features, which we call \textit{anomalies}, and
offering a systematic and reproducible strategy for
future application tests, without
requiring per application effort.

In  this paper, we document the development and implementation of a new
approach to finding and preserving anomalies that we call \textit{Simulating
Environmental Anomalies} (SEA). This technique is founded upon the key
insight that problematic environmental properties can often be detected
in the function calls, system
calls, or other interactions an application makes within an
environment. When employing SEA,
an application under test is exposed
to the anomalies unique to a given environment
in such a way that its responses will indicate
potential for failures upon deployment. In this way, developers are given
an easy and inexpensive way to learn from the mistakes of others, and
thus save money and programming hours that otherwise would be spent to
find and fix environmental bugs.

We found SEA capable of finding bugs,
both known and unknown,
in Linux applications ranked
highly on Debian's popularity contest
by implementing it in a proof of concept tool
called {\em CrashSimulator}\footnote{Our approach is
loosely inspired by flight simulators, which test pilot aptitude under a
variety of rare, adverse scenarios (water landings, engine failures,
etc.) before the pilots are certified to work in practice.}
and evaluating its performance~\cite{DebPopCon}.
These applications were chosen
because they are commonly used,
well tested,
and stable, giving CrashSimulator a chance to find new
environmental bugs where it should be the hardest.
Findings from these evaluations included bugs
attributed to
unanticipated file system configurations, file types, and network delays,
and resulted in a variety of failures, including hangs, crashes, and
filesystem damage.  In total, the SEA technique was able to identify 65
bugs with much less
time and effort than would be required to set up real environments and
execute the same applications within them -- illustrating SEA's
usefulness for developers in a real-world setting.

The main contributions in this work can be summarized as follows:

\begin{itemize}

\item{It provides evidence
that previously unanticipated flaws can be created by the interaction
between an application and its environment.}

\item{It introduces \textit{Simulating Environmental Anomalies} (SEA)
as an easy-to-use method for simulating environments
so an application's behavior in those environments
can be assessed before deployment---
without the time and resource costs of
testing in each environment.}

\item{It allows developers to build a corpus of extracted anomalies and thus 
increase their capability to test applications against
problematic environmental aspects without per application effort.}

\item{It demonstrates a new tool, {\em CrashSimulator},
which implements SEA
in order to find previously-undiscovered environmental bugs
in widely deployed and highly tested code.}

\item{It introduces a new technique called {\it process set cloning}
that can generate copies of a running application,
so that users can test debugging hypotheses without damaging the
original.}

\item{It proves the effectiveness
of {\em CrashSimulator}
by showing it can find real bugs in real applications
when used by developers both involved and not involved with the project,
including developers with limited Linux experience.}

\end{itemize}
