%% Chapter 1
\chapter{Introduction}
\label{chap:intro}
\textit{No Battle Plan Survives Contact With the Enemy --- Helmuth von Moltke}

Given the way everything is scaling up we are expecting software to run in
a ever-growing set of increasingly diverse environments.

No matter how well an application is tested before its release,
new bugs always seem to emerge after deployment.
Oracle estimates that 40\% of deployed applications
contain critical defects -- a situation that is compounded
by the fact that deployment
increases the cost to fix these flaws by 100 fold~\cite{OracleAppQuality}.
One reason for this outcome
is that these applications will operate within a diverse set of
deployment \emph{environments},
and variations between these environments tend to
reveal previously undiscovered flaws.
These flaws emerge from
such factors as
operating system APIs changing across versions
~\cite{LinuxGlibcChanges, WinAPICompat, MuslDifferences},
or small variations in file systems exhibiting subtle but critical
differences~\cite{EXT4Layout, AppleHFS, WindowsNTFS}.
Even if the network and adapter are identical,
network behavior can still diverge from what is expected~\cite{vbox,
NMAPOSDifferences, VMWareNATFailure},
and these environmental differences greatly exacerbate
the chance that an application will function incorrectly when deployed.

These unforeseen bugs
complicate the work of 43\% of application developers who, according to a
recent survey conducted by ClusterHQ~\cite{ClusterHQSurvey},
spend between 10\% and 25\% of their time
debugging errors that only appear in production.
Numerous efforts have been made to reduce this burden.
One approach
is to hide environmental differences behind standard interfaces.
Unfortunately,
even specialized ``Write-Once, Run Anywhere'' environments
that attempt to hide these differences,
such as the Java Runtime Environment,
are not perfect,
leading them to be rechristened ``Write-Once, Debug Everywhere''~\cite{WODE}.
A more direct approach would be
to identify and fix deficiencies before deployment,
but history has shown that,
even if enormous effort is put forward,
it may be insufficient to uncover these bugs.
Microsoft employs thousands of engineers with nearly a
1:1 ratio of testers to developers~\cite{Page2009}.
Yet, a recent Windows Update released
in response to the Spectre Intel CPU vulnerability
resulted in machines with certain hardware configurations
being rendered unbootable~\cite{kb4056892}.

What is needed is a methodical way to record, preserve, and test against
specific features of any environment proven to have caused incorrect
behavior in applications.
What is needed is a way to cataloge these features,
known as ``anomalies,''
so that they may be used to systematically and reproducabily test applications.
In this thesis we discuss the tools and techniques we created in order to achieve this end.

\section{The SEA Technique and Crash Simulator}
The first major component of this work is a novel technique we refer to as Simulating Environmental Anomalies (SEA).
This technique is based on our finding that problematic features of an environment can often be identified in the results and side effects of the function calls, system calls, or other environmental interactions an application makes.
Once defined,
we created a proof-of-concept implementation of SEA called CrashSimulator which applies the technique on the system calls made by Linux applications.
The tool allows users to define ``checkers'',
which passively identify problems in recorded system call streams,
and ``mutators'' which modify a system call recording so that it contains an anomaly.
CrashSimulator can then replay this modified stream in order to see whether or not
an application responds to the anomaly.
Using CrashSimulator's checkers and mutators we were able to find high-impact bugs in widely deployed applications.

\section{Pattern Observation Recognition and Transformation}
The second component is a new domain specific language that is specifically tailored toward making the expression of anomalies easier for developers.
The language,
known as PORT,
came about as a result of findings from a CrashSimulator user study that showed users were having difficulty writing checkers and mutators using a general purpose programming language.
It is inspired by stream and event processing languages and offers a simple,
but powerful syntax for describing both opportunities to simulate an anomaly and what modifications must be made to a stream in order to do so.
Further, PORT is able to extend the SEA technique to other domains by support for a variety of other ``activity representations'' such as XMLRPC requests, and USB traffic.
PORT allowed us to rewrite many of our existing CrashSimulator checkers and mutators in a simpler and more maintainable fashion.
Further, we were able to write programs that can detect and simulate USB-based attacks and hardware bugs.

\section{Organization of the thesis}
\label{sec:organization}
Chapter~\ref{chap:background}
presents an overview of environmental bugs including where they
appear and how they can be detected.
Chapter~\ref{chap:sea}
discusses CrashSimulator -- our implementation of the SEA
technique over system calls.
Chapter~\ref{chap:userstudy}
covers a user study conducted with CrashSimulator that
verified some of the tool's strengths and pointed out some weaknesses.
Chapter~\ref{chap:port}
discusses PORT,
our new domain specific language that addresses these weaknesses by to make
it easy to find and modify interesting portions of streams of
application activity.
Chapter~\ref{chap:relatedwork}
compares our tools and techniques to related work.
Finally,
Chapter~\ref{chap:conclusion} offers our closing thoughts and hopes for
the future of this work.
